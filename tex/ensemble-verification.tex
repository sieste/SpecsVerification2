\documentclass[article]{jss}\usepackage{graphicx, color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0, 0, 0}
\newcommand{\hlnumber}[1]{\textcolor[rgb]{0.501960784313725,0,0.501960784313725}{#1}}%
\newcommand{\hlfunctioncall}[1]{\textcolor[rgb]{0,0.501960784313725,0.752941176470588}{\textbf{#1}}}%
\newcommand{\hlstring}[1]{\textcolor[rgb]{0.650980392156863,0.52156862745098,0}{#1}}%
\newcommand{\hlkeyword}[1]{\textcolor[rgb]{0.733333333333333,0.474509803921569,0.466666666666667}{\textbf{#1}}}%
\newcommand{\hlargument}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlcomment}[1]{\textcolor[rgb]{1,0.501960784313725,0}{#1}}%
\newcommand{\hlroxygencomment}[1]{\textcolor[rgb]{1,0.501960784313725,0}{#1}}%
\newcommand{\hlformalargs}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hleqformalargs}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlassignement}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlpackage}[1]{\textcolor[rgb]{0.501960784313725,0.501960784313725,0.752941176470588}{\textbf{#1}}}%
\newcommand{\hlslot}[1]{\textcolor[rgb]{0,0,0}{\textit{#1}}}%
\newcommand{\hlsymbol}[1]{\textcolor[rgb]{1,0,0.501960784313725}{#1}}%
\newcommand{\hlprompt}[1]{\textcolor[rgb]{0,0,0}{#1}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Stefan Siegert\\University of Exeter}
\title{Verification Functions for Ensemble Forecasts Implemented in the \proglang{R} package \pkg{SpecsVerification}}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Stefan Siegert} %% comma-separated
\Plaintitle{SpecsVerification: New R Functions for Verification of Ensemble Forecasts}
\Shorttitle{\pkg{SpecsVerification}: R Functions for Ensemble Verification} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{

}
\Keywords{keywords, comma-separated, not capitalized, \proglang{Java}}
\Plainkeywords{keywords, comma-separated, not capitalized, Java} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Stefan Siegert\\
  Exeter Climate Systems\\
  College for Engineering, Mathematics, and Physical Sciences\\
  University of Exeter\\
  Exeter, EX4 4QF, United Kingdom\\
  E-mail: \email{Stefan.Siegert@exeter.ac.uk}\\
  URL: \url{http://emps.exeter.ac.uk/mathematics/staff/ss610}
}


%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% initialise R session and knitr


\IfFileExists{upquote.sty}{\usepackage{upquote}}{}


\begin{document}

%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.
% \section[About Java]{About \proglang{Java}}
%% Note: If there is markup in \(sub)section, then it has to be escape as above.


\section{Introduction}

{\bf Ensemble forecasting general}
An ensemble forecast is a collection of forecasts for the same target.
The ensemble members usually differ in the initial conditions, boundary conditions, model physics and background information.
Ensemble forecasting is today operationally used to quantify forecast uncertainty.
Since ensembles are composed of multiple runs of deterministic models, but are used to provide uncertainty information, they can be regarded as a hybrid of deterministic and probabilistic forecasts.
To evaluate the quality of ensemble forecasts, the forecasts have to be compared to their verifying observations of the real world.



{\bf Forecast verification general}
The comparison of forecasts with their verifying observations is commonly referred to as forecast verification \citep{jolliffe2012forecast}.
A verification measure is thus a function that depends on the collection $D=\{x_t, y_t\}_{t=1}^N$ of past forecasts $x_t$ and verifying observations $y_t$.
The collection $D$ is also called a hindcast data set.
A scoring rule is a function $s(x_t, y_t)$ that assigns a real number to a forecast-observation pair.
A general convention that the best possible forecast achieves a score of zero, and lower scores indicate ``better'' forecasts.
The empirical score $1/N \sum_t s(x_t, y_t)$ can be used as a verification measure.
There verification measures that are not scoring rules; examples include the correlation coefficient, and the area under the ROC curve.
There are different scoring rules and verification measures for deterministic forecasts and probabilistic forecasts.
Since ensembles are interpreted as a hybrid, both types of verification measures can be applied to ensembles.


{\bf Finite size effect in ensemble verification}
It is known that verification measures can depend on the ensemble size.
In general, large ensembles achieve better average verification scores than small ensembles, when both ensembles come from the same ensemble forecasting system \citep{buizza1998impact}.
In hindcast experiments it is often desirable to estimate the finite-ensemble effect on the verification score. 
For example, an experimental model configuration might be run at a small ensemble size to save computational resources. 
The skill calculated for the hindcast experiment is not representative of the skill that an operational ensemble with many members would achieve.
A number of verification scores have been proposed in the past to estimate the finite ensemble effect, and correct it.
These will be summarised in the present paper, and are implemented as R functions in the package \pkg{SpecsVerification}.


{\bf Comparative verification}
The value of a verification score is often of little interest by itself.
If the value of the score is not zero, we know that the forecast was not perfect.
But a score different from zero is often difficult to interpret without a reference point.
For this reason, forecasts should be evaluated in a comparative way, by subjecting the forecast of interest to a simple reference forecast, such as the climatological average, or a simple time-series model.
If the forecast is able to achieve a better score than the reference forecast, there is non-trivial information in the forecast.
Forecast verification therefore often includes comparative measures of forecast performance.
The package \pkg{SpecsVerification} includes functions to compare forecast verification scores.


{\bf Uncertainty quantification}
If the score was calculated from a finite number of forecast-observation pairs,  the value of a verification score is of little use without a measure of uncertainty.





\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlfunctioncall{data}(eurotempforecast)
R   <- \hlfunctioncall{ncol}(ens)
yrs <- \hlfunctioncall{as.numeric}(\hlfunctioncall{names}(obs))
N   <- \hlfunctioncall{length}(obs)
\end{alltt}
\end{kframe}
\end{knitrout}



\begin{figure}
\begin{center}
%
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlfunctioncall{par}(las=1, cex=0.7, mgp=\hlfunctioncall{c}(3, 1, 0), mar=\hlfunctioncall{c}(2,4,1,1))
\hlfunctioncall{matplot}(yrs, ens, ylab=\hlstring{"temp [C]"}, pch=1, col=\hlfunctioncall{gray}(.5))
\hlfunctioncall{points}(yrs, obs, pch=15, cex=1.5)
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/gfs-plot} 

\end{knitrout}

%
\end{center}
\caption{Seasonal European temperature forecasts by NCEP CFSv2, initialised in May, verified in JJA.}
\label{gfs-plot}
\end{figure}


\section{Ensemble-adjusted verification scores}

\subsection{Representation of ensemble and observation data}

In \pkg{SpecsVerification}, archives of $N$ instances of ensemble forecasts, each with $R$ members, are represented by $N\times R$ matrices.
The data shown in Figure \ref{gfs-plot} is a matrix with dimensions

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlfunctioncall{dim}(ens)
\end{alltt}
\begin{verbatim}
## [1] 27 24
\end{verbatim}
\end{kframe}
\end{knitrout}



\subsection{Binary forecasts}

This section outlines the theory behind ensemble-adjusted verification scores, using probabilistic forecasts of binary events for illustration.  

One of the most common verification measures for probabilistic forecasts of binary events is the Brier score \citep{brier1950verification}.
Suppose a probability forecast $p_t \in [0,1]$ is issued at time $t$ for a binary (yes/no) event.
The occurrence or non-occurrence of the event is coded as $y_t=1$ or $y_t=0$, respectively. 
The Brier score is given by the squared difference between forecast and observation:
%
\begin{equation}
s_{B}(p_t, y_t) = (p_t - y_t)^2
\end{equation}
%
The Brier score is negatively oriented - lower scores indicate better forecasts.
The Brier score is a strictly proper verification score, meaning that the expected score obtains its minimum value if and only if the observation $y_t$ is a random draw from $p_t$ \citep{gneiting2007strictly}.


Assume next that instead of predicting the probability $p_t$, we make a prediction based on an ensemble forecast of size $R$, whose members were sampled identically and independently with probability $p_t$.
That is, each of the $R$ ensemble members is an independent Bernoulli trial with success probability $p_t$.
An unbiased estimator of the success probability $p_t$ is given by the fraction $i_t/R$, where $i_t$ is the number of successes, i.e. the number of ensemble members that predict the event $y_t=1$.
The Brier score of the estimated probability is equal to
%
\begin{equation}
s_{B}\left(\frac{i_t}{R}, y_t\right) = \left(\frac{i_t}{R} - y_t\right)^2
\label{eq:unfair-brier}
\end{equation}
%
Taking expectation over the random variable $i_t \sim Binomial(p_t, R)$, it is shown that \citep{ferro2008effect}
%
\begin{equation}
E\left[s_{B}\left(\frac{i_t}{R}, y_t\right)\right] = s_{B}(p_t, y_t) +\frac{p_t(1-p_t)}{R}
\label{eq:expec-brier}
\end{equation}
%
That is, even though the fraction $i_t/R$ is an unbiased estimator of the event probability $p_t$, the Brier score of $i_t/R$ is not an unbiased estimator of the Brier score of $p_t$.
The Brier score of $i_t/R$ is in general larger than the Brier score of $p_t$.
The bias, given by the additional positive term on the rhs of Equation~\ref{eq:expec-brier}, depends on the ensemble size and vanishes for $R\rightarrow\infty$.
The bias can thus be interpreted as a finite-ensemble penalty: If two ensembles sample their members from the same probability $p_t$, the one with the larger ensemble size obtains the lower (i.e. better) Brier score on average.
This is reasonable since more ensemble members allow for more robust estimation of the ``true'' probability $p_t$.
But in the analysis of ensemble hindcasts it is sometimes desirable to correct the finite-ensemble bias.
For example, if the hindcast ensemble has $R$ members, and future operational forecasts will be made with $R^* > R$ ensemble members, the score calculated for the $R$-member hindcast ensemble will be a too pessimistic estimate of the score of the $R^*$-member forecast ensemble.


The ensemble-adjusted Brier score, given by \citep{ferro2008effect}
%
\begin{equation}
s_{B}^*(i_t, R, R^*, y_t) = \left(\frac{i_t}{R} - y_t\right)^2 - \frac{i_t(R-i_t)}{R(R-1)}\left(\frac{1}{R} - \frac{1}{R^*}\right)
\label{eq:ens-brier}
\end{equation}
%
allows to correct the finite-ensemble bias.
The ensemble-adjusted Brier score is is in expectation equal to the Brier score that would be achieved by an ensemble with $R^*$ members sampled from the same probability $p_t$, i.e., 
%
\begin{equation}
E\left[s_{B}^*(i_t, R, R^*, y_t)\right] = (p_t - y_t)^2 + \frac{p_t(1-p_t)}{R^*}.
\end{equation}
%
Note that, trivially, $s_{B}^*(i_t, R, R, y_t) = s_{B}(i_t/R, y_t)$.
Note further that setting $R^*=\infty$ yields the fair Brier score \citep{ferro2013fair} which estimates the score of the underlying probability $p_t$.
The ensemble-adjusted Brier score can be used to compare ensemble forecasting systems with different numbers of members.
It further allows for the extrapolation of the average score of an ensemble forecast system to larger ensemble sizes.


The \pkg{SpecsVerification} function \code{EnsBrier} calculates the ensemble-adjusted Brier scores of a collection of $N$ ensemble forecasts and their corresponding binary observations. 
The argument \code{R.new} allows for estimation of the score of an arbitrary ensemble size, including \code{R.new=Inf}.


We have transformed the continuous temperature data into binary forecasts by addressing the question "Will this year's summer be warmer than last year's"?
To illustrate the finite ensemble effect and highlight the benefit of the finite-ensemble adjustment, we randomly split the 24-member forecast ensemble into a small 5-member ensemble and a larger 19-member ensemble.
We then calculate their unadjusted Brier scores:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
i.small <- \hlfunctioncall{sample}(1:R, 5)
i.large <- \hlfunctioncall{setdiff}(1:R, i.small)
\hlfunctioncall{c}(small.ens=\hlfunctioncall{mean}(\hlfunctioncall{EnsBrier}(ens.bin[, i.small], obs.bin)), 
  large.ens=\hlfunctioncall{mean}(\hlfunctioncall{EnsBrier}(ens.bin[, i.large], obs.bin)))
\end{alltt}
\begin{verbatim}
## small.ens large.ens 
##    0.1689    0.1465
\end{verbatim}
\end{kframe}
\end{knitrout}


As expected, the large ensemble obtains a better score than the large ensemble.
We next adjust the Brier score for the finite ensemble size, by calculating $s_{B}^*$ using $R^*=20$ for both ensembles:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlfunctioncall{c}(small.ens=\hlfunctioncall{mean}(\hlfunctioncall{EnsBrier}(ens.bin[, i.small], obs.bin, R.new=19)), 
  large.ens=\hlfunctioncall{mean}(\hlfunctioncall{EnsBrier}(ens.bin[, i.large], obs.bin, R.new=19)))
\end{alltt}
\begin{verbatim}
## small.ens large.ens 
##    0.1454    0.1465
\end{verbatim}
\end{kframe}
\end{knitrout}


The scores are very similar; we are able to estimate the score of the 19-member ensemble, using only a 5-member ensemble.
How big is the estimated improvement if we could further increase the ensemble size?
We show this in Figure \ref{fig:ens-brier}, where we plot the ensemble adjusted Brier score over the adjusted ensemble size $R^*$.


\begin{figure}
\begin{center}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/ens-brier} 

\end{knitrout}

\caption{Ensemble-adjusted Brier score as a function of the ensemble size, calculated for a small 5-member ensemble (solid line) and a larger 19-member ensemble (dashed line) produced by the same forecasting system.}
\label{fig:ens-brier}
\end{center}
\end{figure}


\subsection{Categorical forecasts}


Assume the ensemble forecasting system produces an ensemble of categorical rather than binary forecasts.
That is, each ensemble members and the verifying observation falls into one of $K$ classes.
Two types of categorical forecasts can be distinguished: Disjoint categories and nested categories.

Assume the observation assumes on of $K$ possible values, or classes, and a probabilistic forecast $\mathbf{p}_t = (p_{t,1}, \cdots, p_{t,K})$, is issued.
The verifying observation is vector-valued $\mathbf{y}_t$, where the $k$-th element of $\mathbf{y}_t$ is $y_{t,k}=1$ if the $k$-th class is observed, and $y_{t,k}=0$ otherwise.
The quadratic score for such a probability forecast is given by
%
\begin{equation}
s_{Q}(\mathbf{p}_t, \mathbf{y}_t) = \sum_{k=1}^K \left(p_{t,k} - y_{t,k}\right)^2
\end{equation}
%
The quadratic score is simply the sum of Brier scores for the individual categories.
Or stated differently, the Brier score is one-half the quadratic score of a 2-class categorical forecast.

Now assume an $R$-member categorical ensemble forecast $\mathbf{i}_t$ is issued at time $t$, indicating that $i_{t,k}$ out of $R$ ensemble members have predicted the $k$-th category, for $k=1,\cdots,K$.
Using results obtained for the ensemble-adjusted Brier score, the ensemble-adjusted quadratic score is seen to be
%
\begin{equation}
s_{Q}^*(\mathbf{i}_t, R, R^*, \mathbf{y}_t) = \sum_{k=1}^K \left\{ \left(\frac{i_{t,k}}{R} - y_{t,k}\right)^2 - \left(\frac{1}{R} - \frac{1}{R^*}\right) \frac{i_{t,k}(R-i_{t,k})}{R(R-1)}\right\}
\end{equation}
%
The ensemble adjusted quadratic score is implemented as the function \code{EnsQs} in \pkg{SpecsVerification}.


The quadratic score is insensitive to relabelling the $K$ categories.
Sometimes this is undesired in categorical forecasting problems. 
The forecast categories might have a natural ordering, such as low/medium/high intensity of precipitation.
In that case, an order-sensitive score might be desired that penalises forecasts that assign most probability to the ``high'' category more than it penalises forecasts that assign most probability to the ``medium'' category, if the low category verifies. 
A lot of probability in the ``medium'' category can be considered closer to the observation than a lot of probability in the ``high'' category, such that the former forecast can be considered ``better''.
An order-sensitive score is also desired if the forecast categories are nested within one another, e.g., for precipitation amounts categorised into <1mm/<5mm/<20mm/<$\infty$.
In that case, a high probability assigned to the <5mm category should receive less penalty than a high probability assigned to the <20mm category, if the actual amount is <1mm.


The ranked probability score (RPS) is an order-sensitive quadratic score for categorical probability forecasts.
The forecast vector $\mathbf{i}_t$ is transformed to the $K$-element cumulated forecast vector $\mathbf{j}_t$, with $k$-th element equal to $j_{t,k} = \sum_{l=1}^k i_{t,l}$.
Likewise, the cumulated observation vector $\mathbf{z}_t$ has its $k$-th element equal to $z_{t,k} = \sum_{l=1}^k y_{t,l}$.
The RPS is the quadratic score achieved by the cumulative forecast $\mathbf{j}_t$ for the cumulative observation $\mathbf{z}_t$.
Accumulating the elements of $\mathbf{i}_t$ and $\mathbf{y}_t$ nests the $K$ forecast categories within each other. 
The forecast is transformed from ``$i_{t,k}$ out of $R$ ensemble members predict category $k$'' to the forecast ``$j_{t,k}$ out of $R$ ensemble members forecast category $k$ \emph{or less}''.
The nesting of forecast categories ensures order-sensitivity of the score.
Using results from the previous section, we get the ensemble-adjusted RPS
%
\begin{equation}
s_{R}^*(\mathbf{i}_t, R, R^*, \mathbf{y}_t) = \sum_{k=1}^K \left\{ \left(\frac{j_{t,k}}{R} - z_{t,k}\right)^2 - \left(\frac{1}{R} - \frac{1}{R^*}\right) \frac{j_{t,k}(R-j_{t,k})}{R(R-1)}\right\}
\end{equation}
%
The ensemble adjusted RPS is implemented as the function \code{EnsRps} in \pkg{SpecsVerification}.
Note that the ensemble forecast vector $\mathbf{i}_t$ in the above equations is assumed to be a histogram, indicating the number of ensemble members that forecast each class. 
The functions \code{EnsQs} and \code{EnsRps} assume as inputs true categorical ensemble forecasts, i.e. vectors of length $R$ (ensemble size), with entries indicating which class label the ensemble predicts.



We have transformed the continuous ensemble forecasts into categorical forecasts by addressing the question "Will this year's summer temperature be similar to last year's temperature (within a 0.5K range), colder, or warmer?"
For example, the categorical prediction and observation for the year 2000 is

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlfunctioncall{matrix}(\hlfunctioncall{c}(obs.cat[\hlstring{"2000"}], ens.cat[\hlstring{"2000"},]), ncol=1)
\end{alltt}
\begin{verbatim}
##       [,1]
##  [1,]    1
##  [2,]    2
##  [3,]    2
##  [4,]    2
##  [5,]    3
##  [6,]    2
##  [7,]    2
##  [8,]    2
##  [9,]    2
## [10,]    2
## [11,]    1
## [12,]    1
## [13,]    2
## [14,]    2
## [15,]    2
## [16,]    1
## [17,]    2
## [18,]    1
## [19,]    2
## [20,]    1
## [21,]    2
## [22,]    2
## [23,]    2
## [24,]    1
## [25,]    2
\end{verbatim}
\end{kframe}
\end{knitrout}


This transforms the forecast problem into a 3-category problem, which we first evaluate by the ensemble-adjusted quadratic score:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlfunctioncall{mean}(\hlfunctioncall{EnsQs}(ens.cat, obs.cat))
\end{alltt}
\begin{verbatim}
## [1] 0.5782
\end{verbatim}
\end{kframe}
\end{knitrout}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
i.small <- \hlfunctioncall{sample}(1:R, 5)
i.large <- \hlfunctioncall{setdiff}(1:R, i.small)
\hlfunctioncall{rbind}(
small.ens=\hlfunctioncall{mean}(\hlfunctioncall{EnsQs}(ens.cat[, i.small], obs.cat)),
large.ens=\hlfunctioncall{mean}(\hlfunctioncall{EnsQs}(ens.cat[, i.large], obs.cat)))
\end{alltt}
\begin{verbatim}
##             [,1]
## small.ens 0.6519
## large.ens 0.5856
\end{verbatim}
\end{kframe}
\end{knitrout}



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlfunctioncall{rbind}(
small.ens=\hlfunctioncall{mean}(\hlfunctioncall{EnsRps}(ens.cat[, i.small], obs.cat)),
large.ens=\hlfunctioncall{mean}(\hlfunctioncall{EnsRps}(ens.cat[, i.large], obs.cat)))
\end{alltt}
\begin{verbatim}
##             [,1]
## small.ens 0.3719
## large.ens 0.3386
\end{verbatim}
\end{kframe}
\end{knitrout}




\subsection{Continuous forecasts}


If the forecast target is a continuous variable, such as temperature or pressure, the continuous ranked probability score \citep{matheson1976scoring} can be used for forecast verification.
If the forecast for the continuous target $y_t$ is given as a cumulative distribution function $F_t(x)$, the CRPS is given by 
%
\begin{equation}
s_{C}(F_t, y_t) = \int_{-\infty}^\infty dz\ \left|F_t(z) - H(z-y_t)\right|^2
\end{equation}
%
where $H(x)$ is the Heaviside step-function, satisfying $H(x)=1$ for all $x\ge 0$ and $H(x)=0$ otherwise.
Suppose an ensemble forecast $x_t$ with $R$ real-valued members $x_t = \{x_{t,1}, x_{t,2} \dots, x_{t,R}\}$ is issued for the real-valued verifying observation $y_t$.
The ensemble can be transformed into a cdf by taking the empirical distribution function given by 
%
\begin{equation}
\hat{F}_t(z) = \frac{1}{R} \sum_{r=1}^{R} H(z - x_{t,r}).
\end{equation}
%
Using properties of the Heaviside function, it is straightforward to show that the CRPS of the empirical distribution $\hat{F}$ is given by
%
\begin{equation}
s_{C}(\hat{F}_t, y_t) = \frac{1}{R}|x_{t,r}-y_t| - \frac{1}{2R^2} \sum_{r=1}^R \sum_{r'=1}^R |x_{t,r}-x_{t,r'}|.
\end{equation}
%
\citet{fricker2013three} show that the CRPS is sensitive to the ensemble size, and propose the ensemble-adjusted CRPS
%
\begin{equation}
s_{C}^*(x_t, R, R^*, y_t) = \frac{1}{R}\sum_{r=1}^R |x_{t,r} - y_t| - \frac{1}{2R(R-1)}\left(1-\frac{1}{R^*}\right) \sum_{r=1}^R\sum_{r'=1}^R |x_{t,r}-x_{t,r'}|.
\end{equation}
%
The ensemble-adjusted CRPS is, in expectation, equal to the CRPS that the empirical distribution function calculated from an ensemble of size $R^*$ would achieve.
This includes the case $R^*=\infty$, for which the fair CRPS is obtained.
The ensemble-adjusted CRPS is implemented in the \pkg{SpecsVerification} function \code{EnsCrps}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlfunctioncall{rbind}(
  unadjusted = \hlfunctioncall{mean}(\hlfunctioncall{EnsCrps}(ens, obs, R.new=NA)), 
  fair       = \hlfunctioncall{mean}(\hlfunctioncall{EnsCrps}(ens, obs, R.new=Inf))
)
\end{alltt}
\begin{verbatim}
##              [,1]
## unadjusted 0.1381
## fair       0.1329
\end{verbatim}
\end{kframe}
\end{knitrout}



The ensemble adjusted Ignorance score has recently been proposed \citep{siegert2015ignorance}.





\section{Comparative verification and uncertainty quantification}


\subsection{Reference forecast}

The value of a verification score by itself is meaningless.
In order to evaluate the skill of a forecast, its verification score has to be compared to the score achieved by a reference forecast.
For example, if the skill of a state-of-the-art high resolution climate model is evaluated, it is reasonable to compare its verification score to the score achieved by an older climate model, possibly with lower resolution and less physical detail.

In the absence of a dynamical climate model to which the score can be compared, simple statistical benchmark predictions can be used.
A popular simple reference forecast is the climatological forecast, which is only based on the known record of observations, without reference to any numerical forecast model.
\pkg{SpecsVerification} includes the function \code{ClimEns} which transforms a vector of observations into a matrix of climatological ensemble forecasts, including the possibility to leave out the $t$-th observation in the $t$-th climatological ensemble:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
ens.ref     <- \hlfunctioncall{ClimEns}(obs,     leave.one.out=TRUE)
ens.cat.ref <- \hlfunctioncall{ClimEns}(obs.cat, leave.one.out=TRUE)
ens.bin.ref <- \hlfunctioncall{ClimEns}(obs.bin, leave.one.out=TRUE)
\end{alltt}
\end{kframe}
\end{knitrout}


The new data set of climatological ensembles can be used as a reference ensemble to which the numerical forecast ensemble can be compared.
We recommend also considering statistical reference forecasts such as a linear trend or an auto-regressive model, which might be more suitable than the climatological forecast.



\subsection{Mean scores and mean score differences}

Suppose we have calculated two time series $\{s_{1,1}, s_{1,2}, \dots, s_{1,N}\}$ and $\{s_{2,1}, s_{2,2}, \dots, s_{2,N}\}$ of verification scores for two competing forecast systems for the same observation.
\citet{diebold1995comparing} suggest to test the null-hypothesis of equal forecast accuracy using the time series $d_1, \dots, d_N$ of loss differentials $d_t = s_{1,t} - s_{2,t}$. 
Under the assumption of temporal independence of $d_t$, and zero mean of the loss-differential, the test statistic 
%
\begin{equation}
T = \bar{d}\sqrt{\frac{N}{var(d_t)}}
\end{equation}
%
is asymptotically Normally distributed with mean zero and variance one.
This test is implemented in \pkg{SpecsVerification} in the function \code{ScoreDiff}.
The function includes the option to account for autocorrelation of the loss-differential by specifying an effective sample size \code{N.eff}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlfunctioncall{rbind}(
  brier = \hlfunctioncall{ScoreDiff}(\hlfunctioncall{EnsBrier}(ens.bin, obs.bin), 
                    \hlfunctioncall{EnsBrier}(ens.bin.ref, obs.bin)),
  qs    = \hlfunctioncall{ScoreDiff}(\hlfunctioncall{EnsQs}(ens.cat, obs.cat),    
                    \hlfunctioncall{EnsQs}(ens.cat.ref, obs.cat)),
  rps   = \hlfunctioncall{ScoreDiff}(\hlfunctioncall{EnsRps}(ens.cat, obs.cat),   
                    \hlfunctioncall{EnsRps}(ens.cat.ref, obs.cat)),
  crps  = \hlfunctioncall{ScoreDiff}(\hlfunctioncall{EnsCrps}(ens, obs),          
                    \hlfunctioncall{EnsCrps}(ens.ref, obs))
)
\end{alltt}
\begin{verbatim}
##       score.diff score.diff.sd    p.value     ci.L   ci.U
## brier    0.12185       0.04217 0.00192772  0.03921 0.2045
## qs       0.12004       0.08972 0.09046758 -0.05581 0.2959
## rps      0.09753       0.06438 0.06491317 -0.02866 0.2237
## crps     0.09391       0.02400 0.00004543  0.04688 0.1409
\end{verbatim}
\end{kframe}
\end{knitrout}



How do the score differences change if adjust the ensemble size to infinity?

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlfunctioncall{rbind}(
  brier = \hlfunctioncall{ScoreDiff}(\hlfunctioncall{EnsBrier}(ens.bin,     obs.bin, R.new=Inf), 
                    \hlfunctioncall{EnsBrier}(ens.bin.ref, obs.bin, R.new=Inf)),
  qs    = \hlfunctioncall{ScoreDiff}(\hlfunctioncall{EnsQs}(ens.cat,     obs.cat, R.new=Inf), 
                    \hlfunctioncall{EnsQs}(ens.cat.ref, obs.cat, R.new=Inf)),
  rps   = \hlfunctioncall{ScoreDiff}(\hlfunctioncall{EnsRps}(ens.cat,     obs.cat, R.new=Inf), 
                    \hlfunctioncall{EnsRps}(ens.cat.ref, obs.cat, R.new=Inf)),
  crps  = \hlfunctioncall{ScoreDiff}(\hlfunctioncall{EnsCrps}(ens,     obs, R.new=Inf),  
                    \hlfunctioncall{EnsCrps}(ens.ref, obs, R.new=Inf))
)
\end{alltt}
\begin{verbatim}
##       score.diff score.diff.sd    p.value     ci.L   ci.U
## brier    0.11907       0.04206 0.00232081  0.03663 0.2015
## qs       0.11198       0.08959 0.10568147 -0.06362 0.2876
## rps      0.09529       0.06438 0.06942387 -0.03089 0.2215
## crps     0.09050       0.02399 0.00008094  0.04348 0.1375
\end{verbatim}
\end{kframe}
\end{knitrout}



\subsection{Skill scores}

It is common practice to compare scores of competing forecasts by a so-called skill score, which is a normalised mean score difference \citep{wilks2011statistical}.
Denote by $S$ the mean score of the forecast under evaluation, by $S_{ref}$ the mean score of a reference forecast, and by $S_{perf}$ the mean score that would be achieved by the perfect forecaster.
The skill score is then given by the score difference between the reference forecast and the evaluated forecast, normalised by the difference between the reference forecast and the perfect forecast:
%
\begin{equation}
SS = \frac{S_{ref} - S}{S_{ref} - S_{perf}}
\end{equation}
%
The variance of the skill score can be estimated by error propagation as follows
%
\begin{equation}
var(SS) \approx \frac{1}{(S_{ref} - S_{perf})^2} var(S) + \frac{(S - S_{perf})^2}{(S_{ref}-S_{perf})^2} var(S_{ref}) - 2 \frac{S-S_{perf}}{(S_{ref}-S_{perf})^3} cov(S, S_{ref})
\end{equation}
%
where the variances and covariances of the mean scores $S$ and $S_{ref}$ are approximated by the variances and covariances calculated for the individual scores, divided by the sample size.
The skill score is implemented in \pkg{SpecsVerification} in the function \code{SkillScore}, which takes as inputs two vectors of verification scores of the evaluated and the reference forecast, the constant score achieved by a perfect forecaster, as well as a possibly user-defined effective sample size.


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlfunctioncall{rbind}(
  brier = \hlfunctioncall{SkillScore}(\hlfunctioncall{EnsBrier}(ens.bin, obs.bin), 
                     \hlfunctioncall{EnsBrier}(ens.bin.ref, obs.bin)),
  qs    = \hlfunctioncall{SkillScore}(\hlfunctioncall{EnsQs}(ens.cat, obs.cat),    
                     \hlfunctioncall{EnsQs}(ens.cat.ref, obs.cat)),
  rps   = \hlfunctioncall{SkillScore}(\hlfunctioncall{EnsRps}(ens.cat, obs.cat),   
                     \hlfunctioncall{EnsRps}(ens.cat.ref, obs.cat)),
  crps  = \hlfunctioncall{SkillScore}(\hlfunctioncall{EnsCrps}(ens, obs),          
                     \hlfunctioncall{EnsCrps}(ens.ref, obs))
)
\end{alltt}
\begin{verbatim}
##       skillscore skillscore.sd
## brier     0.4680       0.14973
## qs        0.1719       0.13042
## rps       0.2258       0.14624
## crps      0.4048       0.07343
\end{verbatim}
\end{kframe}
\end{knitrout}


The skill scores increase slightly if we adjust the ensemble size to $R^* \to \infty$ for the ensemble forecasts and the climatological reference forecast:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlfunctioncall{rbind}(
  brier = \hlfunctioncall{SkillScore}(\hlfunctioncall{EnsBrier}(ens.bin, obs.bin, R.new=Inf), 
                     \hlfunctioncall{EnsBrier}(ens.bin.ref, obs.bin, R.new=Inf)),
  qs    = \hlfunctioncall{SkillScore}(\hlfunctioncall{EnsQs}(ens.cat, obs.cat, R.new=Inf), 
                     \hlfunctioncall{EnsQs}(ens.cat.ref, obs.cat, R.new=Inf)),
  rps   = \hlfunctioncall{SkillScore}(\hlfunctioncall{EnsRps}(ens.cat, obs.cat, R.new=Inf), 
                     \hlfunctioncall{EnsRps}(ens.cat.ref, obs.cat, R.new=Inf)),
  crps  = \hlfunctioncall{SkillScore}(\hlfunctioncall{EnsCrps}(ens, obs, R.new=Inf), 
                     \hlfunctioncall{EnsCrps}(ens.ref, obs, R.new=Inf))
)
\end{alltt}
\begin{verbatim}
##       skillscore skillscore.sd
## brier     0.4749       0.15475
## qs        0.1665       0.13519
## rps       0.2234       0.14806
## crps      0.4051       0.07602
\end{verbatim}
\end{kframe}
\end{knitrout}



\subsection{Correlation and correlation difference}

The Pearson correlation coefficient is one of the most popular verification criteria, and can be calculated with the built-in \proglang{R} function \code{cor}.
Since uncertainty quantification is often of interest, \pkg{SpecsVerification} provides the function \code{Corr}, which returns a correlation coefficient, a p-value and a confidence interval.
The user can provide the confidence level for the confidence interval, an effective sample size to account for possible auto-correlation in the data:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
ens.mean <- \hlfunctioncall{rowMeans}(ens)
\hlfunctioncall{Corr}(ens.mean, obs)
\end{alltt}
\begin{verbatim}
##        corr     p.value           L           U 
## 0.757095576 0.000002427 0.529391069 0.883049977
\end{verbatim}
\end{kframe}
\end{knitrout}



It is often of interest to compare the correlation coefficients between two forecasts that were issued for the same observation.
The actual difference in correlation is of interest, as well as an estimation of the statistical significance of the correlation difference.
\pkg{SpecsVerification} implements the function \code{CorrDiff} that returns the difference between the correlation of the forecast ensemble \code{ens} and the correlation of a reference forecast ensemble \code{ens.ref}, both of which were issued for the same observation \code{obs}.
The function calculates a p-value using the test by \citet{steiger1980tests} and a confidence interval based on \citet{zou2007toward} are calculated.
Both methods take into account correlation between the two competing forecasts.
For illustration, we evaluate the difference in correlation between the ensemble mean forecast and the persistence forecast:


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlfunctioncall{CorrDiff}(ens.mean, obs.lag, obs)
\end{alltt}
\begin{verbatim}
## corr.diff   p.value         L         U 
##  0.179021  0.029082 -0.005417  0.440518
\end{verbatim}
\end{kframe}
\end{knitrout}





\section{Rank histogram analysis for ensemble forecasts}

The verification rank histogram \citep{hamill2001interpretation} is a non-parametric graphical tool to assess the reliability of an ensemble forecasting system.
For each pair of ensemble forecast and verifying observation, the rank of the observation among the ordered ensemble members is calculated.
In a $R$-member ensemble, the rank is between $1$ and $R+1$.
If the ensemble is a reliable representation of the uncertainty in the observation, the observation should statistically behave like ``just another ensemble member''. 
Each verification rank should therefore be equally likely on average, and the histogram over verification ranks should be flat. 
\pkg{SpecsVerification} contains the function \code{Rankhist} to calculate the verification rank counts for an archive of ensembles and observations.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
rh <- \hlfunctioncall{Rankhist}(ens, obs)
rh
\end{alltt}
\begin{verbatim}
##  [1] 0 2 1 0 2 4 1 1 0 0 0 0 1 2 2 1 3 1 1 0 1 1 0 2 1
\end{verbatim}
\end{kframe}
\end{knitrout}




The function \code{PlotRankhist} plots the rank histogram.
Two plotting modes are available:
\code{mode="raw"} simply plots the rank counts as a bar plot histogram.
\code{mode="prob.paper"} plots the rank counts on probability paper following \citet{broecker2008reliability}. 
Assuming that each rank count has a binomial distribution with success probability $1/(R+1)$ and sample size $N$.
The observed rank count $c_i$ is transformed to the cumulative probability $\nu_i$ under the Binomial distribution.
To test the null-hypothesis of a flat rank histogram, $90$-, $95$-, and $99$-percent prediction intervals are included, corrected for multiple testing.
The interpretation is, given that the null-hypothesis is true, on average 9 out of 10 rank histograms should lie completely inside the $90\%$ prediction interval.



\begin{figure}
\begin{center}
%
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlfunctioncall{PlotRankhist}(rh, mode=\hlstring{"raw"})
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/rank-hist} 

\end{knitrout}

%
\end{center}
\caption{Rank histogram}
\label{fig:rank-hist}
\end{figure}


\begin{figure}
\begin{center}
%
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlfunctioncall{PlotRankhist}(rh, mode=\hlstring{"prob.paper"})
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/rank-hist-pp} 

\end{knitrout}

%
\end{center}
\caption{Rank histogram on probability paper}
\label{fig:rank-hist-pp}
\end{figure}


The function \code{TestRankhist} implements different statistical tests of the null-hypothesis of flat rank histogram.
Flatness of the rank histgram can be assessed by a Pearson $\chi^2$-test \citep{pearson1900criterion}.
Suppose rank $i$ was observed $r_i$ times for $i=1,\dots,R+1$, and define $e_i=N/(R+1)\ \forall\ i$ the expected number of counts if each verification rank were equally likely.
Define further
\begin{equation}
x_i = \frac{r_i - e_i}{\sqrt{e_i}}.
\end{equation}
%
Under the null-hypothesis of equally likely verification ranks, the test statistic
%
\begin{equation}
\chi^2 = \sum_{i=1}^{R+1} x_i^2
\end{equation}
%
has a $\chi^2$-distribution with $R$ degrees of freedom.


\citet{hamill2001interpretation} showed that certain types of violation of ensemble reliability are visible as different patterns in the rank histogram.
In particular, a constant bias of the mean produces sloped rank histograms, and ensembles with insufficient (excessive) ensemble spread produce $\cup$-shaped ($\cap$-shaped) rank histogram.
\citet{jolliffe2008evaluating} showed that the $\chi^2$-test statistic can be decomposed to test for sloped and convex rank histograms specifically, thus increasing the power of the test.
The test requires the definition of suitable contrast vectors $\mathbf{c}$ of length $R+1$, that satisfy $\sum_i c_i = 0$, $\sum_i c_i^2 = 1$, and $\sum_i c_i c_i' = 0$ for every pair of contrasts $\mathbf{c}$ and $\mathbf{c}'$.
Assuming a number of up to $R$ contrast vectors $\mathbf{c}^{(1)}$, $\mathbf{c}^{(2)}$, $\dots$, the test statistics $(\sum_i c^{(k)}_i x_i)^2$ are independently $\chi^2$ distributed with one d.o.f. 
The function \code{TestRankhist} uses a linear and a squared contrast. Defining $J=R+1$, the $i$-th element of the contrast vectors $\mathbf{c}^{(lin)}$ and $\mathbf{c}^{(sq)}$, for $i=1,\cdots,J$ are given by
%
\begin{align}
c^{(lin)}_i & = -\sqrt{\frac{3(J+1)}{J (J-1)}} + i \sqrt{\frac{12}{J^3 - J}}\text{, and}\\
c^{(sq)}_i & =  - \frac{\sqrt{5}  J^2 - \sqrt{5}}{\sqrt{4(J - 2)  (J-1) J (J+1) (J+2)}}+ \left(i - \frac{J+1}{2}\right)^2   \sqrt{\frac{180}{ J^5 - 5 J^3 + 4 J}}.
\end{align}
%
The $\chi^2$ test using the linear contrast is sensitive to sloped rank histgrams, i.e. biased ensembles, while the $\chi^2$ based on the squared contrast is sensitive to convex rank histograms, i.e. over- or under-dispersed ensembles.
\code{TestRankhist} returns the test-statistics and one-sided p-values of the Pearson $\chi^2$ test, and of the two tests based on the contrasts $\mathbf{c}^{(lin)}$ and $\mathbf{c}^{(sq)}$:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlfunctioncall{TestRankhist}(rh)
\end{alltt}
\begin{verbatim}
##                pearson.chi2 jp.slope jp.convex
## test.statistic      23.9259   0.0114  0.005574
## p.value              0.4658   0.9150  0.940485
\end{verbatim}
\end{kframe}
\end{knitrout}


The rank histogram of the temperature ensemble forecast provides no evidence against the null-hypothesis of a reliable ensemble.

\section{Reliability diagrams for probability forecasts}

The reliability diagram is a classical tool to compare probability forecasts of binary events to the verifying binary observations \citep{jolliffe2012forecast}.
The reliability diagram compares the forecast probability to the conditional frequency of the observation, given the forecast.
A forecast is reliable if the forecast probability and conditional event frequency coincide.
Forecast reliability is a reasonable criterion that probability forecasts should satisfy; over all instances that the forecast issued a probability $p$, the event should happen $p\times 100\%$ of the time.

If the forecast issues probabilities that take any value on the unit interval, most forecast probabilities will be issued only once.
To estimate the conditional event frequency in this case the forecasts can be grouped into a finite number of non-overlapping bins.
The average event frequency taken over all instances where the forecast is in a given bin is then taken as an average of the conditional event frequency.
The reliability diagram is a plot of the conditional event frequency over the in-bin average of the forecast probabilities.
\pkg{SpecsVerification} provides the function \code{ReliabilityDiagram} that takes as inputs a collection of probability foredcasts and binary verifying observations, and calculates the reliability diagram for a specified number of equidistant bins, or a user-defined non-equidistant binning.
The consistency resampling method proposed by \citet{broecker2007increasing} is used to estimate the likely spread of the reliability diagrams around the diagonal if the given forecast were, in fact, reliable.


If the \code{plot} argument is set to \code{FALSE}, the \code{ReliabilityDiagram} function returns the quantities necessary to plot the reliability diagram.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
p.bin <- \hlfunctioncall{rowMeans}(ens.bin)
\hlfunctioncall{ReliabilityDiagram}(p.bin, obs.bin, plot=FALSE, bins=3)
\end{alltt}
\begin{verbatim}
##   p.avgs cond.probs cbar.lo cbar.hi p.counts bin.lower bin.upper
## 1 0.1713     0.2222  0.0000  0.4784        9    0.0000    0.3333
## 2 0.5833     0.4286  0.1667  1.0000        7    0.3333    0.6667
## 3 0.8447     1.0000  0.6000  1.0000       11    0.6667    1.0000
\end{verbatim}
\end{kframe}
\end{knitrout}


If the argument \code{plot=TRUE} the reliability diagram is plotted, as shown in Figure \ref{reldiag-plot}. 
The logical argument \code{plot.refin} controls the refinement diagram, i.e. the histogram over the forecast probabilities.
The logical argument \code{attributes} controls plotting of the polygon defined by the vertical no-resolution line at $\bar{y} = 1/n \sum_t y_t$, where $y_t$ is the binary observation at time $t$, and the no-skill line defined by the linear equation $f(x)=(x+\bar{y})/2$, to produce the attributes diagram \citep{hsu1986attributes}.
Points that fall into the shaded area of the attributes diagram contribute positively to forecast skill defined by the Brier skill score.

\begin{figure}
\begin{center}
%
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
rd <- \hlfunctioncall{ReliabilityDiagram}(p.bin, obs.bin, plot=TRUE, bins=3, attributes=TRUE)
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/reldiag} 

\end{knitrout}

%
\end{center}
\caption{Reliability diagram}
\label{reldiag-plot}
\end{figure}


\section{Additional functions}

The focus of \pkg{SpecsVerification}, and of the present paper, is on verification functions for ensemble and probability forecasts, uncertainty quantification, and graphical display.
\pkg{SpecsVerification} includes a number of additional functions for recalibration and verification of forecasts.
For verification of deterministic forecasts, e.g. the ensemble mean forecast, the sqared error (function \code{SqErr}) and the Absolute Error (function \code{AbsErr}) have been implemented.





\section{Conclusion}

\section*{Acknowledgments}


\bibliography{ensemble-verification}

\end{document}
